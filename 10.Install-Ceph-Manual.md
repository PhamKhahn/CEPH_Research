

## Cài đặt CEPH mimic


## 1. Mô hình

- Mô hình ## chưa cập nhật 
![](images/8.png)


- Network Plan ## chưa cập nhật 
![](images/5.png)



## 2. Cấu hình môi trường

- Cấu hình SeLinux
```
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config
setenforce 0
```

- Cấu hình FileHost
```
cat <<EOF> /etc/hosts

192.168.30.145 ceph_node1
192.168.30.146 ceph_node2
192.168.30.147 ceph_node3

EOF
```

- Cấu hình FirewallD
```
firewall-cmd --add-port=6789/tcp --permanent 
firewall-cmd --add-port=6800-7100/tcp --permanent
firewall-cmd --reload  
```


- Cấu hình NTP
```
yum install -y ntp ntpdate ntp-doc
ntpdate -qu 0.centos.pool.ntp.org 1.centos.pool.ntp.org 2.centos.pool.ntp.org
systemctl start ntpd
systemctl enable ntpd
timedatectl set-ntp true 
hwclock  -w 
```

- Khởi tạo Yum Repository
```
cat <<EOF> /etc/yum.repos.d/ceph.repo
[ceph]
name=Ceph packages for $basearch
baseurl=https://download.ceph.com/rpm-mimic/el7/x86_64/
enabled=1
priority=2
gpgcheck=1
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-mimic/el7/noarch
enabled=1
priority=2
gpgcheck=1
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-source]
name=Ceph source packages
baseurl=https://download.ceph.com/rpm-mimic/el7/SRPMS
enabled=0
priority=2
gpgcheck=1
gpgkey=https://download.ceph.com/keys/release.asc
EOF
```


## 3. Cài đặt và cấu hình Ceph

### 3.1 Cài đặt Ceph trên 3 node

- Cài đặt Yum ` yum-plugin-priorities.`
```
yum -y install yum-plugin-priorities

```
- Cài đặt CEPH package
```
yum -y install snappy leveldb gdisk python-argparse gperftools-libs
yum install -y  ceph-deploy ceph
yum install -y qemu-kvm qemu-kvm-tools qemu-img  qemu-guest-agent qemu-guest-agent-win32


```


## 3.2 Khởi tạo Cluster trên node 1

- Mỗi Cluster yêu cầu ít nhất một MON và các OSD đi kèm.  Bootstrap MON là yêu cầu đầu tiên để khởi động một Ceph Cluster. MON cũng yêu cầu tăng tính khả dụng ca như các thnàh phần khác trong Cluster như Object, pool replica. 


- Để bootstrap một MON cần các cấu hình tối thiểu 
    -  Unique Identifier : ID của cụm 
    -  Cluster Name : tên của cụm 
    -  Monitor Name : mỗi MON instance trong cụm sẽ có 1 tên riêng
    -  Monitor Map : khi bootstrap  một MON đầu tiên sẽ yêu cẩu khởi tạo một monitor map 
    -  Monitor Keyring: monitor nói chuyện với nhau thông qua các secret key. 
    -  Administrator Keyring: sử dụng để làm việc vơi CLuster thông qua admin use và key 


- Khởi tạo fsid, sử dụng cho trường fsid
```
[]# uuidgen
719216c2-fac6-42af-b620-1facfd797d04

[]# UUID=719216c2-fac6-42af-b620-1facfd797d04


```

- Khởi tạo một MON là yêu cầu đầu tiên và tối thiểu để chạy Ceph cluster. Cấu hình node1 làm MON. Khởi tạo cấu hình Ceph
```
cat <<EOF> /etc/ceph/ceph.conf
[global]
fsid = 719216c2-fac6-42af-b620-1facfd797d04
mon initial members = ceph_node1
mon host = 192.168.30.145
public network = 192.168.30.0/24
cluster network = 192.168.50.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd journal size = 1024
osd pool default size = 3
osd pool default min size = 2
osd pool default pg num = 333
osd pool default pgp num = 333
osd crush chooseleaf type = 1
EOF
```

- Khởi tạo Key cho MON
```
ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
```

- Khởi tạo key cho Administrator
```
sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'
```

- Khởi tạo key cho boostrap OSD
```
sudo ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd'

```

- Thêm key vào `ceph.mon.keyringf`
```
sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
```

- Khởi tạo monitor map 
```
monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap

monmaptool --create --add ceph_node1 192.168.30.145 --fsid $UUID /tmp/monmap

```


- Khởi tạo default data directory cho  MON `ceph_node1`
```
sudo -u ceph mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}

sudo -u ceph mkdir /var/lib/ceph/mon/ceph-ceph_node1

```


- Phân quyền 
```
chown -R ceph:ceph /tmp/ceph.mon.keyring
chown -R ceph:ceph /tmp/monmap
```



- Khởi tạo MON ban đầu từ key và mon map
```
sudo -u ceph ceph-mon [--cluster {cluster-name}] --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring

sudo -u ceph ceph-mon --mkfs -i ceph_node1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring

```


- Khởi động monitor
```
touch /var/lib/ceph/mon/ceph-ceph_node1/done

systemctl stop ceph-mon@ceph_node1
systemctl start ceph-mon@ceph_node1
systemctl enable ceph-mon@ceph_node1

systemctl status ceph-mon@ceph_node1

```


### 3.3. Thêm OSD BlueStone trên Ceph_node1 


- Cấu hình Ceph OSD
```

ceph-volume lvm zap /dev/vdb

sudo ceph-volume lvm prepare --data /dev/vdb

sudo ceph-volume lvm list ## return ID and osd FSID
[block]    /dev/ceph-c4922ed5-e8c9-43f3-b71e-7d6eace01c72/osd-block-7c8947dd-300a-43c9-a0a5-d56f47377e07

      type                      block
      osd id                    0
      cluster fsid              69624950-e1a4-4048-9e14-deafee51a943
      cluster name              ceph
      osd fsid                  7c8947dd-300a-43c9-a0a5-d56f47377e07
      encrypted                 0
      cephx lockbox secret      
      block uuid                4AXOTY-VvrD-mc0V-RK6n-slrE-9OPy-JmEJQW
      block device              /dev/ceph-c4922ed5-e8c9-43f3-b71e-7d6eace01c72/osd-block-7c8947dd-300a-43c9-a0a5-d56f47377e07
      vdo                       0
      crush device class        None
      devices                   /dev/vdb

sudo ceph-volume lvm activate {ID} {FSID} ## active OSD
sudo ceph-volume lvm activate 0 7c8947dd-300a-43c9-a0a5-d56f47377e07
```



### 3.4. Thêm MDS trên Ceph_node1


- Khởi tạo thư mục cho MDS
```
mkdir -p /var/lib/ceph/mds/{cluster-name}-{id}

sudo -u ceph mkdir -p /var/lib/ceph/mds/ceph-ceph_node1

```

- Khởi tạo key
```
ceph-authtool --create-keyring /var/lib/ceph/mds/{cluster-name}-{id}/keyring --gen-key -n mds.{id}

ceph-authtool --create-keyring /var/lib/ceph/mds/ceph-ceph_node1/keyring --gen-key -n mds.ceph_node1

```


- Khởi tạo user và import key đã tao
```
ceph auth add mds.{id} osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/{cluster}-{id}/keyring

ceph auth add mds.ceph_node1 osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/ceph-ceph_node1/keyring

```


- Thêm Section MDS trong `/etc/ceph/ceph.conf` . Với ID là hostname của node
```
[mds.{id}]
host = {id}

[mds.ceph_node1]
host = ceph_node1

```

- Khởi dộng MDS Service
```
ceph-mds --cluster {cluster-name} -i {id} -m {mon-hostname}:{mon-port} [-f]

ceph-mds --cluster ceph -i ceph_node1 -m 192.168.30.145:6789 -f

```

## 4. Thêm Node vào Cluster

- Thêm node vào cụm thực hiện copy cấu hình và các key bao gồm client.admin. client.bootstrap-osd tới node mới
```
scp /etc/ceph/ceph.conf root@ceph_node2:/etc/ceph/ceph.conf
scp /etc/ceph/ceph.client.admin.keyring root@ceph_node2:/etc/ceph/ceph.client.admin.keyring
scp /var/lib/ceph/bootstrap-osd/ceph.keyring root@ceph_node2:/var/lib/ceph/bootstrap-osd/ceph.keyring

```


## 5. Thêm OSD vào Cluster trên Ceph_node2


- Khởi tạo OSD

```

ceph-volume lvm zap /dev/vdb 
sudo ceph-volume lvm create --data /dev/vdb
```


## 10. Tài liệu 

- http://docs.ceph.com/docs/mimic/install/manual-deployment/#monitor-bootstrapping