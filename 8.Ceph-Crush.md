

## CRUSH 


## 1. Intro to CRUSH 

- CRUSH bao gồm một map mô tả mô hình vật lý của Ceph Cluster và các rule mà cách dữ liệu được đặt xuống các thiết bị trong mô hình này. 
- CRUSH Map bao gồm ít nhất một hệ thống phân cấp gồm "node"  và "lá".  Node được gọi là bucket , bao gồm tập hợp các vị trí lưu trữ mà được định nghĩa bởi type.  Lá trong crush map gồm một storage device trong danh sách các storage device. Một lá luôn luôn được nằm trong một bucket.  CRUSH map cũng bao gồm các list rule mà định nghĩa cách CRUSH lưu trữ và xuất dữ liệu
- Thuật toán CRUSH thực hiện phân tán dữ liệu trên các storage device dựa vào các weigh value của chúng,. CRUSH thực hiện phân tán và sao chép lưu trữ trên cluster dựa vào các cluster map do người quản trị định nghĩa..  Để ánh xạ placement group tới các OSD thông qua failure domain hay performance domain , CUSH map sẽ định nghĩa một danh sách phân cấp gồm các bucket type. Failure domain gồm : hosts, chassis, racks, power distribution units, pods, rows, rooms, and data centers.. Performance fomain gồm failure domain và OSD 
![](https://access.redhat.com/webassets/avalon/d/Red_Hat_Ceph_Storage-3-Storage_Strategies_Guide-en-US/images/61f559b0c4ab3da4e48ce8a70dfeb4cf/Ceph_Strategies-Guide_459708_1017_01.png)


## 2. Placing Data Dynamically

- Ceph client vào Ceph OSD đều sử dụng CRUSH map và thuật toán của nó
	- Ceph client : crush cho phép ceph client giao tiếp trực tiếp với các OSD, tránh khỏi việc làm việc thông qua các centralized hay broker. ảnh hướng đến hiệu năng - nghẽn cổ chai  hay xuất hiện một single point of failure trong hệ thống. 
	- Ceph OSD : Ceph OSD sẽ sử dụng CRUSH để biết thông tin về hệ thống, , được sử dụng cho việc re-balance, replication, backfilling và recovery và thay thế công việc này cho các Ceph client

## 3. Establishing Failure Domains
- In [computer networking](https://en.wikipedia.org/wiki/Computer_networking "Computer networking"), a **failure domain** encompasses a section of a network that is negatively affected when a critical device or network service experiences problems
- Để giảm giải năng mất mát dữ liệu  đồng thời trên các failure domain, có thể xây dựng các bản sao dữ liệu được đặt trên kệ, rack, nguồn, controller, vị trí địa lý khác nhau . 

## 4. Establishing Performance Domains
- Ceph có thể hỗ trợ nhiều hệ thống phân cấp bằng cách chia  theo các type của phần cứng - chủ yếu dựa vào hiệu suất của phần cứng. CRUSH có thể xây dưngj một hệ thống phân cấp cho HDD và một hệ thống phân cấp khác cho SSD. 

## 5. CRUSH LOCATION

- Các vị trí của object được đặt trong hệ thống trong hệ thống phân cấp của CRUSH map được gọi là `crush localtion`. Các vị trí này bao gồm các cặp key=value mô tả vị trí 
```
root=default row=a rack=a2 chassis=a2a host=a2a1
```

-  Keyname của vị trí được quy định bởi CRUSH type. Mặc định bao gồm : root, datacenter, room, row, pod, pdu, rack, chassis and host
- Mặc định localtion của các object được đặc tại ``root=default  host=HOSTNAME``

## 6. CRUSH STRUCTURE

- Device trong CRUSH MAP gồm : `ceph-osd` daemons hoặc chia theo các device class
- Bucket trong CRUSH MAP bao gồm các type : osd (or device), host, chassis, rack, row, pdu, pod, room, datacenter, region, root. 
- Các Bucket sẽ có các weigh được liên kết, trong các bucket sẽ có các giá trị riêng lẻ cho từng osd ( device ) của chúng.  Các weigh trong này cho biết tương đối tổng dữ liệu mà device hoặc bucket có thể lưu trữ. Weigh của các bucket sẽ bằng weigh các osd ( device ) được chứa bởi nó cộng lại . Thông thường đơn vị của weigh sẽ là terabytes (TB). . Ví dụ 1 weigh sẽ bằng 1 TB

![](images/24.png)

- CRUSH map của cluster : bao gồm class , weigh  và hệ thống phân cấp 
```
[]# ceph osd crush tree
ID CLASS WEIGHT  TYPE NAME           
-1       0.09760 root default        
-3       0.04880 host ceph_node1 
 0   hdd 0.04880         osd.0       
-5       0.04880 host ceph_node2 
 1   hdd 0.04880         osd.1       
```

## 7 . CRUSH Set

### 7.1. Rule
-  Rule định nghĩa các chính sách cách mà dữ liệu được phân tán trên các storage device có trong crush map . Crush định nghĩa các chính sach cho placement,  replica hoặc phân tán để xác định được cách dữ liệu được đặt vào trong hồ chứa
- Rule có thể được định nghĩa thông qua CLI hoặc file cấu hình bằng cách chỉnh pool type ( replicated và erasure coded) , failure domain và device class. 
- Xem rule được định nghĩa trong cụm
```
[]# ceph osd crush rule ls
replicated_rule

```

- Dump content của rule 
```
[]# ceph osd crush rule dump
[
    {
        "rule_id": 0,
        "rule_name": "replicated_rule",
        "ruleset": 0,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -1,
                "item_name": "default"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    }
]

```

### 7.2. Device Classs
- Mặc định OSD được set device class khi chúng khởi động bao gồm hdd, ssd,  nvme dựa vào backend của chúng. 
- 
- 
- Để thay đổi device class thực hiện hoá device class trước
```
ceph osd crush rm-device-class <osd-name> [...]

[]# ceph osd crush rm-device-class osd.0
done removing class of osd(s): 0


```
- Tuỳ chỉnh device class  của OSD
```
ceph osd crush set-device-class <class> <osd-name> [...]

[]# ceph osd crush set-device-class ssd osd.0
set osd(s) 0 to class 'ssd'

```

### 7.3. WEIGHT SET

- Weigh  set được sử dụng khi tính toán vị trí của dữ liệu . Các weigh được liên kết với mỗi device trong CRUSH map dựa vào device size và bao nhiêu data được đặt trên đó. 

- Có  2 kiểu weigh được hỗ trợ : 
	- **compat** weight : một danh sach các weigh trên từng device và node trong cluster. Không phù hợp vì mỗi pool sẽ có số placement groups khác nhau
	- **per-pool** weight : cho phép đặt weigh trên từng pool . 


## 8. MODIFYING THE CRUSH MAP

- Thêm và move OSD trên Clustermap
```
ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]

[]# ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1

```

- Chỉnh sửa OSD weigh
```
ceph osd crush reweight {name} {weight}

[]# ceph osd crush reweight osd.0 2.0
```

- Xoá OSD khỏi map
```
ceph osd crush remove osd.0
```

- Thêm bucket vào map
```
ceph osd crush add-bucket {bucket-name} {bucket-type}


[]# ceph osd crush add-bucket dc_hn datacenter
```

- Move bucket
```
ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]

[]# ceph osd crush move dc_hn rack=rack1
```

- Xoá bucket
```
ceph osd crush remove {bucket-name}

[]# ceph osd crush remove rack1 rack2
```

- Khởi tạo và xoá compat weigh
```
ceph osd crush weight-set create-compat
ceph osd crush weight-set reweight-compat {name} {weight}

[]# ceph osd crush weight-set reweight-compat osd.0 0.48


ceph osd crush weight-set rm-compat
```

- Khởi tạo pool weigh : Bao gồm 2 mode : flat - một weigh duy nhất cho mỗi device và bucket.  và _positional_ - weigh cho mỗi device và bucket ## Minic not supported
```
ceph osd crush weight-set create {pool-name} {mode}


[]# ceph osd crush weight-set create rbd_pool_1 positional
```


- Trong một repicate pool, điều quan trọng là xác định được failure domain. Ví dụ nếu host được chọn là failure domain thì Crush sẽ thực hiện repicate  trên một host khác, tương tụ với rack. 
- Khởi tạo rule cho replicated pool. 
```

```
