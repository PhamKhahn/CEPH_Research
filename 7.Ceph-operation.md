

## 1. HIGH-LEVEL OPERATIONS


### 1.1. Operating Cluster 


- Xem status một MON
```
systemctl status ceph-osd@id_mon
```

- Stop các Ceph daemon
```
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target


```


- Bật tất cả service
```
systemctl start ceph.target

```

- Bật service theo type daemon
```
systemctl start ceph-osd.target
systemctl start ceph-mon.target
systemctl start ceph-mds.target
```


- Xem các Ceph Service đang chạy trên node
```
systemctl status ceph\*.service ceph\*.target

```


### 1.2. Cluster Health Check 

- Kiêm tra LOG cho OSD
```
/var/log/ceph/ceph-osd.*

```


- Xem dung lượng của OSD và Pool 
```
[root@ceph_client ~]# ceph df

GLOBAL:
    SIZE        AVAIL      RAW USED     %RAW USED 
    100 GiB     98 GiB      2.0 GiB          2.01 
POOLS:
    NAME         ID     USED     %USED     MAX AVAIL     OBJECTS 
    rbd_pool     1       0 B         0        31 GiB           0 

```


- Xem trạng thái của các Pool
```
osd pool stats
```

- Kiểm tra các thông số cụ thể trên các pool
```
ceph osd dump 
```

- Xem quota của các Pool
```
[root@ceph_client ~]# ceph df detail
GLOBAL:
    SIZE        AVAIL      RAW USED     %RAW USED     OBJECTS 
    100 GiB     98 GiB      2.0 GiB          2.01          0  
POOLS:
    NAME         ID     QUOTA OBJECTS     QUOTA BYTES     USED     %USED     MAX AVAIL     OBJECTS     DIRTY     READ     WRITE     RAW USED 
    rbd_pool     1      N/A               N/A              0 B         0        31 GiB           0        0       0 B       0 B          0 B 
```


- Cấu hình quota
```
ceph osd pool set-quota <poolname> max_objects <num-objects>
ceph osd pool set-quota <poolname> max_bytes <num-bytes>

ceph osd pool set-quota rbd_pool max_bytes 1024

```

- Hiển thị trạng thái Cluster
```
ceph -s / ceph status

ceph health

ceph health detail ## only warning and error status


```


### 1.3.Monitoring  Cluster


- Hiển thị trạng thái Cluster
```
ceph -s / ceph status

ceph health

ceph health detail ## only warning and error status


```


- Xem log của cả cụm, dữ liệu được xuất từ /etc/ceph/ceph.loh
```
ceph -wc
```

- Kiểm tra dữ liệu được replicate và mở rộng trên các pool
```
ceph df 
```

- Kiểm tra Ratio
```
ceph osd dump | grep full_ratio

```

- Hiểm thị các ODS dưới dạng tree
```
osd osd tree
```

- Kiểm tra trạng thái các MON
```
osd mon stat 

ceph mon dump

```

- Kiểm tra quorum status của cluster
```
quorum_status
```

- Kiểm trang mạng thái MDS
```
ceph mds stat
```

- Kiểm tra trạng thái MDS cluster
```
ceph fs dump 
```


### 1.4. MONITORING OSDS AND PGS

- Ceph data placement giúp các dữ liệu được truyền vào cụm không bị rằng buộc một địa chỉ trực tiếp trên các OSD chungs được đặt trên.  Việc theo dõi dữ liệu trên cả cụm sẽ liên quan đến OSD và PGS là chủ yếu./ 


- Một OSD sẽ bao gồm 2 trạng thái trọng cụm in - the cluster và out - the cluster , bên cạnh đó sẽ có 2 trạng thái service up running và down - not running. Trong trường hợp OSD đi vào trạng thái out khỏi cụm, Ceph sẽ di dời placement groups sang OSD khác. 


- List OSD ỏ trạng thái up
```
ceph osd status
```

- List tất cả các OSD
```
ceph osd tree
```

- Khởi động OSD ở trạng thái down
```
systemctl start ceph-osd@1
```

- Khi CRUSH thực hiện chỉ định  placement group cho các OSD, nó sẽ xem xét số repicate của pool, và gán các placement cho các OSD sao cho mỗi bảo sao của placement group được gán trên các OSD khác nhau. Ví dụ nếu pool yêu cầu 3 bản replica cho các placment group, CRUSH sẽ chỉ định chúng vào osd.1, osd.2 and osd3. 




- Liệt kê các placement group
```
ceph pg dump

```

- Trạng thái :
    - ACTIVE : Once Ceph completes the peering process, a placement group may become active. The active state means that the data in the placement group is generally available in the primary placement group and the replicas for read and write operations.
    -   CLEAN : When a placement group is in the clean state, the primary OSD and the replica OSDs have successfully peered and there are no stray replicas for the placement group. Ceph replicated all objects in the placement group the correct number of times.


### 1.5 : USER MANAGEMENT

- Khi Ceph chạy ở mode yêu cầu xác thực, nếu không chỉ định user. `client.admin ` sẽ được chỉ định là user mặc định. và sẽ sử dụng `keyring` trong /etc/ceph nếu không chỉ định keyring.

```
ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health

```

- Pool được chỉ ddinhj trong Ceph để thực hiện viết, đọc dữ liệu không bất bể Ceph Client nào. Các user được ủy nhiệm đặc quyền sẽ được ceph cho phép làm việc với các pool này. và dữ liệu trong nó. Với ceph khái niệm người dùng được quản lý được xếp vào "client". Form user trong CEPH : TYPE.ID


- Ceph sử dụng "capabilities" để thể hiện các quyền được mà các user có thể làm việc với các MON, OSD, metadata server. 
```
{daemon-type} '{cap-spec}[, {cap-spec} ...]'

```

- http://docs.ceph.com/docs/mimic/rados/operations/user-management/#background


- Xem user trên cluster
```
ceph auth ls

```

- Xóa một user
```
ceph auth del  {TYPE}.{ID}
```


- Khi khởi tạo một user, cần cung cấp user key cho Ceph client, để có có thể xác thực trong cụm
```
ceph-authtool --create-keyring /path/to/keyring

```

- Khởi tạo 1 user
```
ceph auth add client.nguyenhungsync mon 'allow r' osd 'allow rw pool=rbd_pool'

```


-  Khởi tạo Key
```
sudo ceph-authtool -C /etc/ceph/ceph.keyring
```

- Import key cho user
```
ceph auth get client.nguyenhungsync -o /etc/ceph/ceph.client.admin.keyring
```

- Xem các cap của user
```

ceph auth get client.nguyenhungsync

```